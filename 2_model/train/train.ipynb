{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e21b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class SQLiDetector:\n",
    "    \"\"\"SQL Injection Detection Model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='random_forest'):\n",
    "        self.model_type = model_type\n",
    "        self.model = self._initialize_model(model_type)\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def _initialize_model(self, model_type):\n",
    "        \"\"\"Initialize ML model\"\"\"\n",
    "        models = {\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=20,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=5,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'naive_bayes': MultinomialNB(alpha=0.1)\n",
    "        }\n",
    "        \n",
    "        if model_type not in models:\n",
    "            raise ValueError(f\"Model type must be one of {list(models.keys())}\")\n",
    "        \n",
    "        return models[model_type]\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(f\"Training {self.model_type} model...\")\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Training metrics\n",
    "        y_train_pred = self.model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        if X_val is not None and y_val is not None:\n",
    "            y_val_pred = self.model.predict(X_val)\n",
    "            val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, output_path='.'):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        \n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nTest Metrics:\")\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(f\"ROC-AUC:   {self.metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, \n",
    "                                   target_names=['Normal', 'SQLi']))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Normal', 'SQLi'],\n",
    "                   yticklabels=['Normal', 'SQLi'])\n",
    "        plt.title(f'Confusion Matrix - {self.model_type}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # --- PATH FIXED ---\n",
    "        # Use the provided output_path argument\n",
    "        plt.savefig(f'{output_path}/confusion_matrix_{self.model_type}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict on new data\"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_feature_importance(self, feature_names, top_n=20, output_path='.'):\n",
    "        \"\"\"Get feature importance (for tree-based models)\"\"\"\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            importances = self.model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:top_n]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(f'Top {top_n} Feature Importances - {self.model_type}')\n",
    "            plt.barh(range(top_n), importances[indices])\n",
    "            plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # --- PATH FIXED ---\n",
    "            # Use the provided output_path argument\n",
    "            plt.savefig(f'{output_path}/feature_importance_{self.model_type}.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            return dict(zip([feature_names[i] for i in indices], \n",
    "                          importances[indices].tolist()))\n",
    "        return None\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        joblib.dump(self.model, f\"{path}/model_{self.model_type}.pkl\")\n",
    "        \n",
    "        # Save metrics\n",
    "        self.metrics['model_type'] = self.model_type\n",
    "        self.metrics['timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        with open(f\"{path}/metrics_{self.model_type}.json\", 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        # Note: You'll need to pass the model_type to the constructor\n",
    "        # before calling load, as the path depends on it.\n",
    "        self.model = joblib.load(f\"{path}/model_{self.model_type}.pkl\")\n",
    "\n",
    "\n",
    "# --- PATHS FIXED ---\n",
    "# Default paths are now relative to the project root (e.g., C:\\Projects\\SqliDev)\n",
    "def train_all_models(data_path='1_data/processed', output_path='2_model/models'):\n",
    "    \"\"\"Train and compare multiple models\"\"\"\n",
    "    \n",
    "    print(\"Loading processed data...\")\n",
    "    X_train = joblib.load(f\"{data_path}/X_train.pkl\")\n",
    "    X_val = joblib.load(f\"{data_path}/X_val.pkl\")\n",
    "    X_test = joblib.load(f\"{data_path}/X_test.pkl\")\n",
    "    y_train = joblib.load(f\"{data_path}/y_train.pkl\")\n",
    "    y_val = joblib.load(f\"{data_path}/y_val.pkl\")\n",
    "    y_test = joblib.load(f\"{data_path}/y_test.pkl\")\n",
    "    \n",
    "    # Load feature names\n",
    "    with open(f\"{data_path}/feature_info.json\", 'r') as f:\n",
    "        feature_info = json.load(f)\n",
    "    feature_names = feature_info['text_features'] + feature_info['manual_features']\n",
    "    \n",
    "    # Combine train and val for final training\n",
    "    X_train_full = vstack([X_train, X_val])\n",
    "    y_train_full = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    print(f\"\\nFull training set: {X_train_full.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Train multiple models\n",
    "    model_types = ['random_forest', 'gradient_boosting', 'logistic_regression']\n",
    "    results = {}\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {model_type.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        detector = SQLiDetector(model_type=model_type)\n",
    "        detector.train(X_train_full, y_train_full)\n",
    "        \n",
    "        # --- PATHS FIXED ---\n",
    "        # Pass the output_path to the evaluate method\n",
    "        metrics = detector.evaluate(X_test, y_test, output_path=output_path)\n",
    "        \n",
    "        # Get feature importance\n",
    "        if model_type in ['random_forest', 'gradient_boosting']:\n",
    "            # --- PATHS FIXED ---\n",
    "            # Pass the output_path to the get_feature_importance method\n",
    "            importance = detector.get_feature_importance(feature_names, output_path=output_path)\n",
    "        \n",
    "        detector.save(output_path)\n",
    "        results[model_type] = metrics\n",
    "    \n",
    "    # Compare models\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    comparison_df = []\n",
    "    for model_type, metrics in results.items():\n",
    "        comparison_df.append({\n",
    "            'Model': model_type,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1_score'],\n",
    "            'ROC-AUC': metrics['roc_auc']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_df)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_df.to_csv(f\"{output_path}/model_comparison.csv\", index=False)\n",
    "    \n",
    "    # Select best model based on F1-score\n",
    "    best_model_type = max(results.items(), key=lambda x: x[1]['f1_score'])[0]\n",
    "    print(f\"\\nBest model: {best_model_type}\")\n",
    "    \n",
    "    # Save best model info\n",
    "    with open(f\"{output_path}/best_model.txt\", 'w') as f:\n",
    "        f.write(best_model_type)\n",
    "    \n",
    "    return results, best_model_type\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This will now use the corrected default paths\n",
    "    results, best_model = train_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
