{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2578507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ADD THESE 3 LINES ---\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- ADD THIS BLOCK TO FIX THE IMPORT ---\n",
    "# Get the directory of the current script (evaluate/)\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Go up one level (to 2_model/)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Define the path to the 'train' directory\n",
    "train_dir = os.path.join(parent_dir, 'train')\n",
    "\n",
    "# Add the 'train' directory to the Python path\n",
    "if train_dir not in sys.path:\n",
    "    sys.path.append(train_dir)\n",
    "# --- END OF FIX ---\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_curve, roc_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This line will now work\n",
    "from train import SQLiDetector\n",
    "\n",
    "\n",
    "def evaluate_detailed(model_path='2_model/models', data_path='1_data/processed'):\n",
    "    \"\"\"Detailed model evaluation\"\"\"\n",
    "    \n",
    "    # Load best model\n",
    "    with open(f\"{model_path}/best_model.txt\", 'r') as f:\n",
    "        best_model_type = f.read().strip()\n",
    "    \n",
    "    print(f\"Loading {best_model_type} model...\")\n",
    "    detector = SQLiDetector(model_type=best_model_type)\n",
    "    detector.load(model_path)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test = joblib.load(f\"{data_path}/X_test.pkl\")\n",
    "    y_test = joblib.load(f\"{data_path}/y_test.pkl\")\n",
    "    X_test_raw = joblib.load(f\"{data_path}/X_test_raw.pkl\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = detector.predict(X_test)\n",
    "    y_pred_proba = detector.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # --- FIX 2: LOAD SAVED METRICS ---\n",
    "    # The detector.metrics object is empty on a fresh load.\n",
    "    # We must load the metrics saved by train.py.\n",
    "    with open(f\"{model_path}/metrics_{best_model_type}.json\", 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    roc_auc = metrics.get('roc_auc', 0)  # Get the saved ROC-AUC score\n",
    "    # --- END OF FIX 2 ---\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, label=f'AP = {ap_score:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_path}/precision_recall_curve.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # --- Use the loaded roc_auc value ---\n",
    "    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})') \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_path}/roc_curve.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Error Analysis - False Positives and False Negatives\n",
    "    fp_indices = np.where((y_test == 0) & (y_pred == 1))[0]\n",
    "    fn_indices = np.where((y_test == 1) & (y_pred == 0))[0]\n",
    "    \n",
    "    print(f\"\\nFalse Positives: {len(fp_indices)}\")\n",
    "    print(\"Sample FP queries:\")\n",
    "    for idx in fp_indices[:5]:\n",
    "        print(f\"  - {X_test_raw[idx]} (prob: {y_pred_proba[idx]:.3f})\")\n",
    "    \n",
    "    print(f\"\\nFalse Negatives: {len(fn_indices)}\")\n",
    "    print(\"Sample FN queries:\")\n",
    "    for idx in fn_indices[:5]:\n",
    "        print(f\"  - {X_test_raw[idx]} (prob: {y_pred_proba[idx]:.3f})\")\n",
    "    \n",
    "    # Save error analysis\n",
    "    error_analysis = {\n",
    "        'false_positives': [\n",
    "            {'query': X_test_raw[idx], 'probability': float(y_pred_proba[idx])}\n",
    "            for idx in fp_indices\n",
    "        ],\n",
    "        'false_negatives': [\n",
    "            {'query': X_test_raw[idx], 'probability': float(y_pred_proba[idx])}\n",
    "            for idx in fn_indices\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Note: 'import json' was moved to the top\n",
    "    with open(f'{model_path}/error_analysis.json', 'w') as f:\n",
    "        json.dump(error_analysis, f, indent=2)\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_detailed()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
